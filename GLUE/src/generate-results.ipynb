{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate results from GLUE evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and adaptation results on GLUE have already been obtained, we simply need to collect the results from the different log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_METRICS = {\"cola\": \"matthews_correlation\", \n",
    "                \"mnli\": \"accuracy\", \n",
    "                \"mrpc\": \"combined_score\", \n",
    "                \"qnli\": \"accuracy\", \n",
    "                \"qqp\": \"combined_score\", \n",
    "                \"rte\": \"accuracy\", \n",
    "                \"sst2\": \"accuracy\", \n",
    "                \"stsb\": \"combined_score\", \n",
    "                \"wnli\": \"accuracy\"}\n",
    "\n",
    "UNIMODAL_MODELS = [\"bert-base-uncased\"]\n",
    "MULTIMODAL_MODELS = [\"clipbert\", \"lxmert\", \"visualbert\"]\n",
    "MULTIMODAL_ADAPTATIONS = [\"-avg-visual-features\", \n",
    "                          \"-finetuned-lxmert-visual-features\", \n",
    "                          \"-finetuned-wikipedia-visual-features\", \n",
    "                          \"-no-visual-features\",\n",
    "                          \"-no-visual-features-finetuned-lxmert\", \n",
    "                          \"-no-visual-features-finetuned-wikipedia\", \n",
    "                          \"-zero-image-visual-features\",\n",
    "                          \"-zeroed-visual-features\"]\n",
    "MODEL_SPECIFIC_MULTIMODAL_ADAPTATIONS = {\"clipbert\": [\"-imagined-visual-features\"],\n",
    "                                         \"lxmert\": [],\n",
    "                                         \"visualbert\": []\n",
    "                                        }\n",
    "UNIMODAL_ADAPTATIONS = [\"\", \"-trained-lxmert\", \"-trained-lxmert-scratch\", \"-trained-wikipedia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_score(filename, task):\n",
    "    with open(filename, 'r') as f:\n",
    "        scores = json.load(f)\n",
    "    metric_name = \"eval_\"+TASK_METRICS[task]\n",
    "    return scores[metric_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_filenames(dirname):\n",
    "    eval_filenames = {}\n",
    "    for file in os.listdir(dirname):\n",
    "        if \"GLUE-benchmark-\" in file:\n",
    "            # folders look as follows: 'GLUE-benchmark-rte-bert-base-uncased-2022-05-02T09-38'\n",
    "            task_name = file.split(\"-\")[2]\n",
    "            eval_filename = os.path.join(dirname, file, \"eval_results.json\")\n",
    "            if os.path.exists(eval_filename):\n",
    "                if task_name in eval_filenames:\n",
    "                    raise ValueError(f\"Duplicate entries for task {task_name} found in {dirname}\")\n",
    "                else:\n",
    "                    eval_filenames[task_name] = eval_filename\n",
    "    assert eval_filenames.keys() == TASK_METRICS.keys(), \"All eval task files should be present in the given folder\"\n",
    "    return eval_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_dirnames(dirname):\n",
    "    model_dirnames = {}\n",
    "    for model in UNIMODAL_MODELS:\n",
    "        for adaptation in UNIMODAL_ADAPTATIONS:\n",
    "            model_name = model+adaptation\n",
    "            model_dirname = os.path.join(dirname, model_name)\n",
    "            assert os.path.exists(model_dirname), f\"The directory '{model_dirname}' should exist\"\n",
    "            model_dirnames[model_name] = model_dirname\n",
    "    for model in MULTIMODAL_MODELS:\n",
    "        for adaptation in MULTIMODAL_ADAPTATIONS:\n",
    "            model_name = model+adaptation\n",
    "            model_dirname = os.path.join(dirname, model_name)\n",
    "            assert os.path.exists(model_dirname), f\"The directory '{model_dirname}' should exist\"\n",
    "            model_dirnames[model_name] = model_dirname\n",
    "        for adaptation in MODEL_SPECIFIC_MULTIMODAL_ADAPTATIONS[model]:\n",
    "            model_name = model+adaptation\n",
    "            model_dirname = os.path.join(dirname, model_name)\n",
    "            assert os.path.exists(model_dirname), f\"The directory '{model_dirname}' should exist\"\n",
    "            model_dirnames[model_name] = model_dirname\n",
    "            \n",
    "    return model_dirnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnli_eval_results(dirname, logname_starter):\n",
    "    if logname_starter is None:\n",
    "        raise ValueError(\"logname_starter cannot be None\")\n",
    "    eval_acc = []\n",
    "    values_found = 0\n",
    "    for file in os.listdir(dirname):\n",
    "        if file[:6] == logname_starter and file[-6:] == \"_1.out\":\n",
    "            with open(os.path.join(dirname, file), \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    if \" eval_accuracy \" in line:\n",
    "                        accuracy_part = line.split()[-1]\n",
    "                        eval_acc.append(float(accuracy_part))\n",
    "                        values_found += 1 \n",
    "    assert values_found == 2, f\"There should be two mnli eval_accuracy values in {dirname}\"\n",
    "    return eval_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"model\", \"task\", \"score\"])\n",
    "logname_starter = None # fill in this - prefix of generated lognames from `benchmark_model_GLUE.py`, needed for extracting evaluation results from generated logfile of run\n",
    "\n",
    "model_dirnames = get_model_dirnames(\"GLUE/data/logs\")\n",
    "for model, dirname in model_dirnames.items():\n",
    "    eval_filenames = get_eval_filenames(dirname)\n",
    "    for task, eval_filename in eval_filenames.items():\n",
    "        if task == \"mnli\":\n",
    "            # need to specifically handle mnli since eval was screwed up due to several eval sets\n",
    "            mnli_scores = get_mnli_eval_results(dirname, logname_starter)\n",
    "            score = sum(mnli_scores)/2\n",
    "        else:\n",
    "            score = get_eval_score(eval_filename, task)\n",
    "        results = results.append({\"model\": model, \"task\": task, \"score\": score}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>qqp</td>\n",
       "      <td>0.895803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>sst2</td>\n",
       "      <td>0.936927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>stsb</td>\n",
       "      <td>0.883918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>cola</td>\n",
       "      <td>0.611204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>mnli</td>\n",
       "      <td>0.845550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>visualbert-zeroed-visual-features</td>\n",
       "      <td>qnli</td>\n",
       "      <td>0.902618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>visualbert-zeroed-visual-features</td>\n",
       "      <td>sst2</td>\n",
       "      <td>0.911697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>visualbert-zeroed-visual-features</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>0.793530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>visualbert-zeroed-visual-features</td>\n",
       "      <td>rte</td>\n",
       "      <td>0.599278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>visualbert-zeroed-visual-features</td>\n",
       "      <td>wnli</td>\n",
       "      <td>0.563380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model  task     score\n",
       "0                    bert-base-uncased   qqp  0.895803\n",
       "1                    bert-base-uncased  sst2  0.936927\n",
       "2                    bert-base-uncased  stsb  0.883918\n",
       "3                    bert-base-uncased  cola  0.611204\n",
       "4                    bert-base-uncased  mnli  0.845550\n",
       "..                                 ...   ...       ...\n",
       "256  visualbert-zeroed-visual-features  qnli  0.902618\n",
       "257  visualbert-zeroed-visual-features  sst2  0.911697\n",
       "258  visualbert-zeroed-visual-features  mrpc  0.793530\n",
       "259  visualbert-zeroed-visual-features   rte  0.599278\n",
       "260  visualbert-zeroed-visual-features  wnli  0.563380\n",
       "\n",
       "[261 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased</th>\n",
       "      <td>0.805837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased-trained-lxmert</th>\n",
       "      <td>0.781412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased-trained-lxmert-scratch</th>\n",
       "      <td>0.629244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased-trained-wikipedia</th>\n",
       "      <td>0.782761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-avg-visual-features</th>\n",
       "      <td>0.765934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-finetuned-lxmert-visual-features</th>\n",
       "      <td>0.765073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-finetuned-wikipedia-visual-features</th>\n",
       "      <td>0.768888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-imagined-visual-features</th>\n",
       "      <td>0.769229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-no-visual-features</th>\n",
       "      <td>0.765053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-no-visual-features-finetuned-lxmert</th>\n",
       "      <td>0.762133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-no-visual-features-finetuned-wikipedia</th>\n",
       "      <td>0.767967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-zero-image-visual-features</th>\n",
       "      <td>0.765821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clipbert-zeroed-visual-features</th>\n",
       "      <td>0.767653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lxmert-avg-visual-features</th>\n",
       "      <td>0.605915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lxmert-finetuned-lxmert-visual-features</th>\n",
       "      <td>0.607599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lxmert-finetuned-wikipedia-visual-features</th>\n",
       "      <td>0.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lxmert-no-visual-features</th>\n",
       "      <td>0.611199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lxmert-no-visual-features-finetuned-lxmert</th>\n",
       "      <td>0.588693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lxmert-no-visual-features-finetuned-wikipedia</th>\n",
       "      <td>0.611040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lxmert-zero-image-visual-features</th>\n",
       "      <td>0.592722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lxmert-zeroed-visual-features</th>\n",
       "      <td>0.610630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualbert-avg-visual-features</th>\n",
       "      <td>0.781628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualbert-finetuned-lxmert-visual-features</th>\n",
       "      <td>0.770889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualbert-finetuned-wikipedia-visual-features</th>\n",
       "      <td>0.775397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualbert-no-visual-features</th>\n",
       "      <td>0.779843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualbert-no-visual-features-finetuned-lxmert</th>\n",
       "      <td>0.778516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualbert-no-visual-features-finetuned-wikipedia</th>\n",
       "      <td>0.785518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualbert-zero-image-visual-features</th>\n",
       "      <td>0.776462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visualbert-zeroed-visual-features</th>\n",
       "      <td>0.761495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      score\n",
       "model                                                      \n",
       "bert-base-uncased                                  0.805837\n",
       "bert-base-uncased-trained-lxmert                   0.781412\n",
       "bert-base-uncased-trained-lxmert-scratch           0.629244\n",
       "bert-base-uncased-trained-wikipedia                0.782761\n",
       "clipbert-avg-visual-features                       0.765934\n",
       "clipbert-finetuned-lxmert-visual-features          0.765073\n",
       "clipbert-finetuned-wikipedia-visual-features       0.768888\n",
       "clipbert-imagined-visual-features                  0.769229\n",
       "clipbert-no-visual-features                        0.765053\n",
       "clipbert-no-visual-features-finetuned-lxmert       0.762133\n",
       "clipbert-no-visual-features-finetuned-wikipedia    0.767967\n",
       "clipbert-zero-image-visual-features                0.765821\n",
       "clipbert-zeroed-visual-features                    0.767653\n",
       "lxmert-avg-visual-features                         0.605915\n",
       "lxmert-finetuned-lxmert-visual-features            0.607599\n",
       "lxmert-finetuned-wikipedia-visual-features         0.610500\n",
       "lxmert-no-visual-features                          0.611199\n",
       "lxmert-no-visual-features-finetuned-lxmert         0.588693\n",
       "lxmert-no-visual-features-finetuned-wikipedia      0.611040\n",
       "lxmert-zero-image-visual-features                  0.592722\n",
       "lxmert-zeroed-visual-features                      0.610630\n",
       "visualbert-avg-visual-features                     0.781628\n",
       "visualbert-finetuned-lxmert-visual-features        0.770889\n",
       "visualbert-finetuned-wikipedia-visual-features     0.775397\n",
       "visualbert-no-visual-features                      0.779843\n",
       "visualbert-no-visual-features-finetuned-lxmert     0.778516\n",
       "visualbert-no-visual-features-finetuned-wikipedia  0.785518\n",
       "visualbert-zero-image-visual-features              0.776462\n",
       "visualbert-zeroed-visual-features                  0.761495"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby(\"model\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"GLUE/data/results/results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
