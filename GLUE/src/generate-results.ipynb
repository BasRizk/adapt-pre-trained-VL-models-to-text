{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate results from GLUE evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and adaptation results on GLUE have already been obtained, we simply need to collect the results from the different log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home1/brizk/adapt-pre-trained-VL-models-to-text\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_METRICS = {\"cola\": [\"matthews_correlation\"], \n",
    "                \"mnli\": [\"accuracy\"],\n",
    "                \"mnli_mm\": [\"accuracy\"],\n",
    "                \"mrpc\": [\"accuracy\",\"f1\"], \n",
    "                \"qnli\": [\"accuracy\"], \n",
    "                \"qqp\":  [\"accuracy\",\"f1\"], \n",
    "                \"rte\":  [\"accuracy\"], \n",
    "                \"sst2\": [\"accuracy\"], \n",
    "                \"stsb\": [\"spearmanr\"], \n",
    "                \"wnli\": [\"accuracy\"]}\n",
    "\n",
    "UNIMODAL_MODELS = [\"bert-base-uncased\"]\n",
    "MULTIMODAL_MODELS = [\"clipbert\", \"lxmert\", \"visualbert\"]\n",
    "MULTIMODAL_ADAPTATIONS = [\"-avg-visual-features\", \n",
    "                          \"-finetuned-lxmert-visual-features\", \n",
    "                          \"-finetuned-wikipedia-visual-features\", \n",
    "                          \"-no-visual-features\",\n",
    "                          \"-no-visual-features-finetuned-lxmert\", \n",
    "                          \"-no-visual-features-finetuned-wikipedia\", \n",
    "                          \"-zero-image-visual-features\",\n",
    "                          \"-zeroed-visual-features\",\n",
    "                          \"-generated-imgs-features\"]\n",
    "MODEL_SPECIFIC_MULTIMODAL_ADAPTATIONS = {\"clipbert\": [\"-imagined-visual-features\"],\n",
    "                                         \"lxmert\": [],\n",
    "                                         \"visualbert\": []\n",
    "                                        }\n",
    "UNIMODAL_ADAPTATIONS = [\"\", \"-trained-lxmert\", \"-trained-lxmert-scratch\", \"-trained-wikipedia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_score(filename, task):\n",
    "    with open(filename, 'r') as f:\n",
    "        scores = json.load(f)\n",
    "    eval_scores = {}\n",
    "    for metric in TASK_METRICS[task]:\n",
    "        metric_name = \"eval_\"+metric\n",
    "        eval_scores[metric] = scores[metric_name]\n",
    "    return eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_filenames(dirname):\n",
    "    eval_filenames = {}\n",
    "    for file in os.listdir(dirname):\n",
    "        if \"GLUE-benchmark-\" in file:\n",
    "            # folders with run results look as follows: 'GLUE-benchmark-rte-bert-base-uncased-2022-05-02T09-38'\n",
    "            task_name = file.split(\"-\")[2]\n",
    "            eval_filename = os.path.join(dirname, file, (\"_\").join([task_name, \"eval_results.json\"]))\n",
    "            if os.path.exists(eval_filename):\n",
    "                if task_name in eval_filenames:\n",
    "                    raise ValueError(f\"Duplicate entries for task {task_name} found in {dirname}\")\n",
    "                else:\n",
    "                    eval_filenames[task_name] = eval_filename\n",
    "                    # mnli-mm is evaluated together with mnli\n",
    "                    if task_name == \"mnli\":\n",
    "                        eval_filenames[\"mnli_mm\"] = eval_filename.replace(\"mnli_eval_results\", \"mnli_mm_eval_results\")\n",
    "    if not eval_filenames.keys() == TASK_METRICS.keys():                    \n",
    "        print(f\"Warning: All eval task files should be present in the given folder '{dirname}'. Found:\\n{eval_filenames.keys()}\\nShould have:\\n{TASK_METRICS.keys()}\")\n",
    "    return eval_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename(model, adaptation, dirname):\n",
    "    model_name = model+adaptation\n",
    "    model_dirname = os.path.join(dirname, model_name)\n",
    "    \n",
    "    if not os.path.exists(model_dirname):\n",
    "        print(f\"Warning: Missing results, the directory '{model_dirname}' should exist\")\n",
    "        return None, None\n",
    "    return model_name, model_dirname\n",
    "    \n",
    "def get_model_dirnames(dirname):\n",
    "    model_dirnames = {}\n",
    "    for model in UNIMODAL_MODELS:\n",
    "        for adaptation in UNIMODAL_ADAPTATIONS:\n",
    "            model_name, model_dirname = extract_filename(model, adaptation, dirname)\n",
    "            if model_name is not None:\n",
    "                model_dirnames[model_name] = model_dirname\n",
    "    for model in MULTIMODAL_MODELS:\n",
    "        for adaptation in MULTIMODAL_ADAPTATIONS:\n",
    "            model_name, model_dirname = extract_filename(model, adaptation, dirname)\n",
    "            if model_name is not None:\n",
    "                model_dirnames[model_name] = model_dirname\n",
    "        for adaptation in MODEL_SPECIFIC_MULTIMODAL_ADAPTATIONS[model]:\n",
    "            model_name, model_dirname = extract_filename(model, adaptation, dirname)\n",
    "            if model_name is not None:\n",
    "                model_dirnames[model_name] = model_dirname\n",
    "            \n",
    "    return model_dirnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnli_eval_results(dirname, logname_starter):\n",
    "    if logname_starter is None:\n",
    "        raise ValueError(\"logname_starter cannot be None\")\n",
    "    eval_acc = []\n",
    "    values_found = 0\n",
    "    for file in os.listdir(dirname):\n",
    "        if file[:6] == logname_starter and file[-6:] == \"_1.out\":\n",
    "            with open(os.path.join(dirname, file), \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    if \" eval_accuracy \" in line:\n",
    "                        accuracy_part = line.split()[-1]\n",
    "                        eval_acc.append(float(accuracy_part))\n",
    "                        values_found += 1 \n",
    "    assert values_found == 2, f\"There should be two mnli eval_accuracy values in {dirname}\"\n",
    "    return eval_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing results, the directory 'GLUE/data/logs/lxmert-generated-imgs-features' should exist\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/clipbert-generated-imgs-features'. Found:\n",
      "dict_keys(['wnli'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/lxmert-avg-visual-features'. Found:\n",
      "dict_keys(['mrpc', 'qqp', 'stsb', 'cola', 'mnli', 'mnli_mm', 'wnli', 'sst2', 'rte'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/lxmert-finetuned-lxmert-visual-features'. Found:\n",
      "dict_keys(['mnli', 'mnli_mm', 'mrpc', 'cola', 'wnli'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/lxmert-finetuned-wikipedia-visual-features'. Found:\n",
      "dict_keys(['qqp', 'rte', 'sst2', 'wnli', 'cola'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/lxmert-no-visual-features'. Found:\n",
      "dict_keys(['cola', 'mrpc', 'rte'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/lxmert-no-visual-features-finetuned-lxmert'. Found:\n",
      "dict_keys(['sst2', 'stsb', 'qqp', 'rte', 'cola', 'wnli', 'mrpc'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/lxmert-no-visual-features-finetuned-wikipedia'. Found:\n",
      "dict_keys(['mrpc', 'cola', 'sst2', 'qqp', 'wnli', 'rte', 'mnli', 'mnli_mm', 'stsb'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/lxmert-zero-image-visual-features'. Found:\n",
      "dict_keys(['sst2', 'rte', 'stsb', 'wnli', 'qqp', 'cola', 'mrpc'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/lxmert-zeroed-visual-features'. Found:\n",
      "dict_keys(['mnli', 'mnli_mm', 'cola', 'mrpc', 'stsb', 'sst2', 'wnli', 'rte'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/visualbert-no-visual-features'. Found:\n",
      "dict_keys(['cola', 'wnli', 'stsb', 'mrpc', 'rte', 'sst2'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/visualbert-no-visual-features-finetuned-lxmert'. Found:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'wnli', 'qnli', 'mrpc'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/visualbert-no-visual-features-finetuned-wikipedia'. Found:\n",
      "dict_keys(['wnli', 'cola'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/visualbert-zero-image-visual-features'. Found:\n",
      "dict_keys(['mrpc', 'cola', 'rte', 'wnli'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/visualbert-zeroed-visual-features'. Found:\n",
      "dict_keys(['mrpc', 'cola', 'wnli'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n",
      "Warning: All eval task files should be present in the given folder 'GLUE/data/logs/visualbert-generated-imgs-features'. Found:\n",
      "dict_keys(['wnli', 'mrpc'])\n",
      "Should have:\n",
      "dict_keys(['cola', 'mnli', 'mnli_mm', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli'])\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=[\"model\", \"task\", \"metric\", \"score\"])\n",
    "\n",
    "model_dirnames = get_model_dirnames(\"GLUE/data/logs\")\n",
    "for model, dirname in model_dirnames.items():\n",
    "    eval_filenames = get_eval_filenames(dirname)\n",
    "    for task, eval_filename in eval_filenames.items():\n",
    "        score = get_eval_score(eval_filename, task)\n",
    "        for key, val in score.items():\n",
    "            results = results.append({\"model\": model, \"task\": task, \"metric\": key, \"score\": val}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-uncased',\n",
       " 'bert-base-uncased-trained-lxmert',\n",
       " 'bert-base-uncased-trained-lxmert-scratch',\n",
       " 'bert-base-uncased-trained-wikipedia',\n",
       " 'clipbert-avg-visual-features',\n",
       " 'clipbert-finetuned-lxmert-visual-features',\n",
       " 'clipbert-finetuned-wikipedia-visual-features',\n",
       " 'clipbert-generated-imgs-features',\n",
       " 'clipbert-imagined-visual-features',\n",
       " 'clipbert-no-visual-features',\n",
       " 'clipbert-no-visual-features-finetuned-lxmert',\n",
       " 'clipbert-no-visual-features-finetuned-wikipedia',\n",
       " 'clipbert-zero-image-visual-features',\n",
       " 'clipbert-zeroed-visual-features',\n",
       " 'lxmert-avg-visual-features',\n",
       " 'lxmert-finetuned-lxmert-visual-features',\n",
       " 'lxmert-finetuned-wikipedia-visual-features',\n",
       " 'lxmert-no-visual-features',\n",
       " 'lxmert-no-visual-features-finetuned-lxmert',\n",
       " 'lxmert-no-visual-features-finetuned-wikipedia',\n",
       " 'lxmert-zero-image-visual-features',\n",
       " 'lxmert-zeroed-visual-features',\n",
       " 'visualbert-avg-visual-features',\n",
       " 'visualbert-finetuned-lxmert-visual-features',\n",
       " 'visualbert-finetuned-wikipedia-visual-features',\n",
       " 'visualbert-generated-imgs-features',\n",
       " 'visualbert-no-visual-features',\n",
       " 'visualbert-no-visual-features-finetuned-lxmert',\n",
       " 'visualbert-no-visual-features-finetuned-wikipedia',\n",
       " 'visualbert-zero-image-visual-features',\n",
       " 'visualbert-zeroed-visual-features']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(results['model'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_14632168/ipykernel_13490/1592800726.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  results[results['model'].str.contains('visualbert')][results['task'].str.match(task)] #[['model','score']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>visualbert-avg-visual-features</td>\n",
       "      <td>cola</td>\n",
       "      <td>matthews_correlation</td>\n",
       "      <td>0.524198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>visualbert-finetuned-lxmert-visual-features</td>\n",
       "      <td>cola</td>\n",
       "      <td>matthews_correlation</td>\n",
       "      <td>0.503307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>visualbert-finetuned-wikipedia-visual-features</td>\n",
       "      <td>cola</td>\n",
       "      <td>matthews_correlation</td>\n",
       "      <td>0.511034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>visualbert-no-visual-features</td>\n",
       "      <td>cola</td>\n",
       "      <td>matthews_correlation</td>\n",
       "      <td>0.511795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>visualbert-no-visual-features-finetuned-lxmert</td>\n",
       "      <td>cola</td>\n",
       "      <td>matthews_correlation</td>\n",
       "      <td>0.488237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>visualbert-no-visual-features-finetuned-wikipedia</td>\n",
       "      <td>cola</td>\n",
       "      <td>matthews_correlation</td>\n",
       "      <td>0.527953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>visualbert-zero-image-visual-features</td>\n",
       "      <td>cola</td>\n",
       "      <td>matthews_correlation</td>\n",
       "      <td>0.505577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>visualbert-zeroed-visual-features</td>\n",
       "      <td>cola</td>\n",
       "      <td>matthews_correlation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 model  task  \\\n",
       "224                     visualbert-avg-visual-features  cola   \n",
       "244        visualbert-finetuned-lxmert-visual-features  cola   \n",
       "254     visualbert-finetuned-wikipedia-visual-features  cola   \n",
       "258                      visualbert-no-visual-features  cola   \n",
       "265     visualbert-no-visual-features-finetuned-lxmert  cola   \n",
       "273  visualbert-no-visual-features-finetuned-wikipedia  cola   \n",
       "276              visualbert-zero-image-visual-features  cola   \n",
       "281                  visualbert-zeroed-visual-features  cola   \n",
       "\n",
       "                   metric     score  \n",
       "224  matthews_correlation  0.524198  \n",
       "244  matthews_correlation  0.503307  \n",
       "254  matthews_correlation  0.511034  \n",
       "258  matthews_correlation  0.511795  \n",
       "265  matthews_correlation  0.488237  \n",
       "273  matthews_correlation  0.527953  \n",
       "276  matthews_correlation  0.505577  \n",
       "281  matthews_correlation  0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_of_choices=['cola', 'wnli', 'mrpc'] # removed sst2\n",
    "task = task_of_choices[0]\n",
    "results[results['model'].str.contains('visualbert')][results['task'].str.match(task)] #[['model','score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['model', 'metric']\n",
    "results_removed_dups = results.set_index(cols, append=True).drop_duplicates()['score'].copy()\n",
    "results_removed_dups.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(\"model\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(\"model\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"GLUE/data/results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "53b4c6a7f73b52596e80ece9b93385903539f75a6217f81f115b6a651ae58bf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
