{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate results from GLUE evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and adaptation results on GLUE have already been obtained, we simply need to collect the results from the different log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_METRICS = {\"cola\": \"matthews_correlation\", \n",
    "                \"mnli\": \"accuracy\", \n",
    "                \"mnli_mm\": \"accuracy\", \n",
    "                \"mrpc\": \"combined_score\", \n",
    "                \"qnli\": \"accuracy\", \n",
    "                \"qqp\": \"combined_score\", \n",
    "                \"rte\": \"accuracy\", \n",
    "                \"sst2\": \"accuracy\", \n",
    "                \"stsb\": \"combined_score\", \n",
    "                \"wnli\": \"accuracy\"}\n",
    "\n",
    "UNIMODAL_MODELS = [\"bert-base-uncased\"]\n",
    "MULTIMODAL_MODELS = [\"clipbert\", \"lxmert\", \"visualbert\"]\n",
    "MULTIMODAL_ADAPTATIONS = [\"-avg-visual-features\", \n",
    "                          \"-finetuned-lxmert-visual-features\", \n",
    "                          \"-finetuned-wikipedia-visual-features\", \n",
    "                          \"-no-visual-features\",\n",
    "                          \"-no-visual-features-finetuned-lxmert\", \n",
    "                          \"-no-visual-features-finetuned-wikipedia\", \n",
    "                          \"-zero-image-visual-features\",\n",
    "                          \"-zeroed-visual-features\"]\n",
    "MODEL_SPECIFIC_MULTIMODAL_ADAPTATIONS = {\"clipbert\": [\"-imagined-visual-features\"],\n",
    "                                         \"lxmert\": [],\n",
    "                                         \"visualbert\": []\n",
    "                                        }\n",
    "UNIMODAL_ADAPTATIONS = [\"\", \"-trained-lxmert\", \"-trained-lxmert-scratch\", \"-trained-wikipedia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_score(filename, task):\n",
    "    with open(filename, 'r') as f:\n",
    "        scores = json.load(f)\n",
    "    metric_name = \"eval_\"+TASK_METRICS[task]\n",
    "    return scores[metric_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_filenames(dirname):\n",
    "    eval_filenames = {}\n",
    "    for file in os.listdir(dirname):\n",
    "        if \"GLUE-benchmark-\" in file:\n",
    "            # folders with run results look as follows: 'GLUE-benchmark-rte-bert-base-uncased-2022-05-02T09-38'\n",
    "            task_name = file.split(\"-\")[2]\n",
    "            eval_filename = os.path.join(dirname, file, (\"_\").join([task_name, \"eval_results.json\"]))\n",
    "            if os.path.exists(eval_filename):\n",
    "                if task_name in eval_filenames:\n",
    "                    raise ValueError(f\"Duplicate entries for task {task_name} found in {dirname}\")\n",
    "                else:\n",
    "                    eval_filenames[task_name] = eval_filename\n",
    "                    # mnli-mm is evaluated together with mnli\n",
    "                    if task_name == \"mnli\":\n",
    "                        eval_filenames[\"mnli_mm\"] = eval_filename.replace(\"mnli_eval_results\", \"mnli_mm_eval_results\")\n",
    "    if not eval_filenames.keys() == TASK_METRICS.keys():                    \n",
    "        print(f\"Warning: All eval task files should be present in the given folder '{dirname}'. Found only:\\n{eval_filenames.keys()}\\nShould have:\\n{TASK_METRICS.keys()}\")\n",
    "    return eval_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename(model, adaptation, dirname):\n",
    "    model_name = model+adaptation\n",
    "    model_dirname = os.path.join(dirname, model_name)\n",
    "    \n",
    "    if not os.path.exists(model_dirname):\n",
    "        print(f\"Warning: Missing results, the directory '{model_dirname}' should exist\")\n",
    "        return None, None\n",
    "    return model_name, model_dirname\n",
    "    \n",
    "def get_model_dirnames(dirname):\n",
    "    model_dirnames = {}\n",
    "    for model in UNIMODAL_MODELS:\n",
    "        for adaptation in UNIMODAL_ADAPTATIONS:\n",
    "            model_name, model_dirname = extract_filename(model, adaptation, dirname)\n",
    "            if model_name is not None:\n",
    "                model_dirnames[model_name] = model_dirname\n",
    "    for model in MULTIMODAL_MODELS:\n",
    "        for adaptation in MULTIMODAL_ADAPTATIONS:\n",
    "            model_name, model_dirname = extract_filename(model, adaptation, dirname)\n",
    "            if model_name is not None:\n",
    "                model_dirnames[model_name] = model_dirname\n",
    "        for adaptation in MODEL_SPECIFIC_MULTIMODAL_ADAPTATIONS[model]:\n",
    "            model_name, model_dirname = extract_filename(model, adaptation, dirname)\n",
    "            if model_name is not None:\n",
    "                model_dirnames[model_name] = model_dirname\n",
    "            \n",
    "    return model_dirnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnli_eval_results(dirname, logname_starter):\n",
    "    if logname_starter is None:\n",
    "        raise ValueError(\"logname_starter cannot be None\")\n",
    "    eval_acc = []\n",
    "    values_found = 0\n",
    "    for file in os.listdir(dirname):\n",
    "        if file[:6] == logname_starter and file[-6:] == \"_1.out\":\n",
    "            with open(os.path.join(dirname, file), \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    if \" eval_accuracy \" in line:\n",
    "                        accuracy_part = line.split()[-1]\n",
    "                        eval_acc.append(float(accuracy_part))\n",
    "                        values_found += 1 \n",
    "    assert values_found == 2, f\"There should be two mnli eval_accuracy values in {dirname}\"\n",
    "    return eval_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"model\", \"task\", \"score\"])\n",
    "\n",
    "model_dirnames = get_model_dirnames(\"GLUE/data/logs\")\n",
    "for model, dirname in model_dirnames.items():\n",
    "    eval_filenames = get_eval_filenames(dirname)\n",
    "    for task, eval_filename in eval_filenames.items():\n",
    "        score = get_eval_score(eval_filename, task)\n",
    "        results = results.append({\"model\": model, \"task\": task, \"score\": score}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(\"model\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(\"model\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"GLUE/data/results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
