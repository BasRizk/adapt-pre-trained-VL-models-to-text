{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process visual features to get constant visual features for adaptations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Make sure that you are standing in the root folder of the repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs all computations necessary to generate the constant visual features used for the following adaptations:\n",
    "* `avg-visual-features`\n",
    "* `zero-image-visual-features`\n",
    "* `zeroed-visual-features`\n",
    "\n",
    "LXMERT and VisualBERT use the same types of visual features, while CLIP-BERT doesn't. Thus, we generate two separate visual feature versions per adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from adaptations.src.utils import load_obj_tsv\n",
    "import torch\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From average over training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT\n",
    "\n",
    "First, you need to download the image datasets. Do this via the following commands:\n",
    "\n",
    "**MS COCO**\n",
    "\n",
    "Train\n",
    "```bash\n",
    "wget https://nlp.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/train2014_obj36.zip -P adaptations/data/lxmert/mscoco_imgfeat\n",
    "unzip adaptations/data/lxmert/mscoco_imgfeat/train2014_obj36.zip -d adaptations/data/lxmert/mscoco_imgfeat && rm adaptations/data/lxmert/mscoco_imgfeat/train2014_obj36.zip\n",
    "```\n",
    "* 17 GB zipped\n",
    "* 31 GB unzipped and downloaded\n",
    "\n",
    "Validation\n",
    "```bash\n",
    "wget https://nlp.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/val2014_obj36.zip -P adaptations/data/lxmert/mscoco_imgfeat\n",
    "unzip adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.zip -d adaptations/data/lxmert/mscoco_imgfeat && rm adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.zip\n",
    "```\n",
    "* 8.1 GB zipped\n",
    "* 15 GB unzipped and downloaded\n",
    "\n",
    "**Visual Genome**\n",
    "\n",
    "```bash\n",
    "wget https://nlp.cs.unc.edu/data/lxmert_data/vg_gqa_imgfeat/vg_gqa_obj36.zip -P adaptations/data/lxmert/vg_gqa_imgfeat\n",
    "unzip adaptations/data/lxmert/vg_gqa_imgfeat/vg_gqa_obj36.zip -d adaptations/data/lxmert/vg_gqa_imgfeat && rm adaptations/data/lxmert/vg_gqa_imgfeat/vg_gqa_obj36.zip\n",
    "```\n",
    "\n",
    "* 30 GB zipped\n",
    "* 55 GB unzipped and downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import base64\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "FIELDNAMES = [\"img_id\", \"img_h\", \"img_w\", \"objects_id\", \"objects_conf\",\n",
    "              \"attrs_id\", \"attrs_conf\", \"num_boxes\", \"boxes\", \"features\"]\n",
    "\n",
    "def get_tsv_data_item(item):\n",
    "    for key in ['img_h', 'img_w', 'num_boxes']:\n",
    "        item[key] = int(item[key])\n",
    "\n",
    "    boxes = item['num_boxes']\n",
    "    decode_config = [\n",
    "        ('objects_id', (boxes, ), np.int64),\n",
    "        ('objects_conf', (boxes, ), np.float32),\n",
    "        ('attrs_id', (boxes, ), np.int64),\n",
    "        ('attrs_conf', (boxes, ), np.float32),\n",
    "        ('boxes', (boxes, 4), np.float32),\n",
    "        ('features', (boxes, -1), np.float32),\n",
    "    ]\n",
    "    for key, shape, dtype in decode_config:\n",
    "        item[key] = np.frombuffer(base64.b64decode(item[key]), dtype=dtype)\n",
    "        item[key] = item[key].reshape(shape)\n",
    "        item[key].setflags(write=False)\n",
    "        \n",
    "    return item\n",
    "\n",
    "# LXMERT expects normalized boxes (copied from airsplay/lxmert)\n",
    "def get_normalized_boxes(item):\n",
    "    # Normalize the boxes (to 0 ~ 1)\n",
    "    img_h, img_w = item['img_h'], item['img_w']\n",
    "    boxes = item[\"boxes\"].copy()\n",
    "    boxes[:, (0, 2)] /= img_w\n",
    "    boxes[:, (1, 3)] /= img_h\n",
    "    np.testing.assert_array_less(boxes, 1+1e-5)\n",
    "    np.testing.assert_array_less(-boxes, 0+1e-5)\n",
    "    \n",
    "    return boxes\n",
    "    \n",
    "\n",
    "def get_avg_visual_properties_from_files(fnames, features_shape=(36, 2048), pos_shape=(36, 4)):\n",
    "    feature_vector = np.zeros(features_shape)\n",
    "    pos_vector = np.zeros(pos_shape)\n",
    "    num_iters = 0\n",
    "    for fname in fnames:\n",
    "        start_time = time.time()\n",
    "        print(\"Start to load Faster-RCNN detected objects from %s\" % fname)\n",
    "        with open(fname) as f:\n",
    "            reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "            for i, item in enumerate(tqdm(reader)):\n",
    "                item = get_tsv_data_item(item)\n",
    "                feature_vector += item[\"features\"]\n",
    "                pos_vector += get_normalized_boxes(item)\n",
    "                num_iters += 1\n",
    "                \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Loaded file %s in %d seconds.\" % (fname, elapsed_time))\n",
    "    return feature_vector/num_iters, pos_vector/num_iters\n",
    "\n",
    "def get_avg_visual_properties_across_detections_from_files(fnames, features_shape=(1, 2048), pos_shape=(1, 4)):\n",
    "    feature_vector = np.zeros(features_shape)\n",
    "    pos_vector = np.zeros(pos_shape)\n",
    "    num_iters = 0\n",
    "    for fname in fnames:\n",
    "        start_time = time.time()\n",
    "        print(\"Start to load Faster-RCNN detected objects from %s\" % fname)\n",
    "        with open(fname) as f:\n",
    "            reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "            for i, item in enumerate(tqdm(reader)):\n",
    "                item = get_tsv_data_item(item)\n",
    "                feature_vector += np.sum(item[\"features\"], axis=0)\n",
    "                pos_vector += np.sum(get_normalized_boxes(item), axis=0)\n",
    "                num_iters += item[\"features\"].shape[0]\n",
    "                \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Loaded file %s in %d seconds.\" % (fname, elapsed_time))\n",
    "    return feature_vector/num_iters, pos_vector/num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_VAL_FEATURES_PATH = \"adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.tsv\"\n",
    "COCO_TRAIN_FEATURES_PATH = \"adaptations/data/lxmert/mscoco_imgfeat/train2014_obj36.tsv\"\n",
    "VG_FEATURES_PATH = \"adaptations/data/lxmert/vg_gqa_imgfeat/vg_gqa_obj36.tsv\"\n",
    "\n",
    "data_files = [COCO_VAL_FEATURES_PATH, COCO_TRAIN_FEATURES_PATH, VG_FEATURES_PATH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per detection (one different vector for each detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_vector, avg_pos_vector = get_avg_visual_properties_from_files(data_files, features_shape=(36, 2048), pos_shape=(36, 4))\n",
    "\n",
    "avg_feature_tensor = torch.Tensor(avg_feature_vector)\n",
    "torch.save(avg_feature_tensor, os.path.join(\"adaptations/data/avg-visual-features\", \"frcnn_features_per_detection.pt\"))\n",
    "\n",
    "avg_pos_tensor = torch.Tensor(avg_pos_vector)\n",
    "torch.save(avg_pos_tensor, os.path.join(\"adaptations/data/avg-visual-features\", \"frcnn_boxes_per_detection.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = h5py.File(\"models/data/clip-bert/clip_features.hdf5\", mode=\"r\")\n",
    "image_features = buffer[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_vector = torch.zeros((512,))\n",
    "for feats in image_features:\n",
    "    avg_feature_vector += feats\n",
    "avg_feature_vector = avg_feature_vector/len(image_features)\n",
    "torch.save(avg_feature_vector, \"adaptations/data/avg-visual-features/clip_features.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For black (zeroed) image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT and VisualBERT (Faster-R CNN)\n",
    "\n",
    "The `output.csv` file is generated from a Faster-R CNN using the code in `adaptations/data/zero-image-visual-features`.\n",
    "\n",
    "ID translations can be found at \n",
    "* https://github.com/peteanderson80/bottom-up-attention/blob/master/data/genome/1600-400-20/objects_vocab.txt.\n",
    "* https://github.com/peteanderson80/bottom-up-attention/blob/master/data/genome/1600-400-20/attributes_vocab.txt\n",
    "\n",
    "**objects_id**\n",
    "\n",
    "72: \"sky\"\n",
    "\n",
    "956: \"background\"\n",
    "\n",
    "**attrs_id**\n",
    "\n",
    "11: \"black\"\n",
    "\n",
    "163: \"dark\"\n",
    "\n",
    "The Faster-R CNN results seemingly agree with results by Iki et al. for their black image (https://github.com/Alab-NII/eval_vl_glue/blob/main/demo/extractor_demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"adaptations/data/zero-image-visual-features/output.csv\") as f:\n",
    "    reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "    for i, item in enumerate(tqdm(reader)):\n",
    "        assert i < 1\n",
    "        item = get_tsv_data_item(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item[\"features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.Tensor(item[\"features\"]), \"adaptations/data/zero-image-visual-features/frcnn_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_boxes = get_normalized_boxes(item)\n",
    "torch.save(torch.Tensor(normalized_boxes), \"adaptations/data/zero-image-visual-features/frcnn_boxes.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor\n",
    "from PIL import Image\n",
    "\n",
    "from models.src.clip_bert.precompute_clip_visual_features import PatchedCLIPFeatureExtractor, VisualOnlyCLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor.feature_extractor = PatchedCLIPFeatureExtractor(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    pil_image = Image.open(\"adaptations/data/zero-image-visual-features/filled_with_0.png\")\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "    example = processor(images=pil_image, return_tensors=\"pt\")\n",
    "    example[\"pixel_values\"] = example[\"pixel_values\"]\n",
    "\n",
    "    model = VisualOnlyCLIPModel().to(device)\n",
    "    clip_features = model(**example.to(device))\n",
    "    clip_features = clip_features.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clip_features, \"adaptations/data/zero-image-visual-features/clip_features.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero vector as visual features filler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT and VisualBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_features = torch.zeros((36, 2048))\n",
    "visual_boxes = torch.zeros((36, 4))\n",
    "\n",
    "torch.save(visual_features, \"adaptations/data/zeroed-visual-features/frcnn_features.pt\")\n",
    "torch.save(visual_boxes, \"adaptations/data/zeroed-visual-features/frcnn_boxes.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_features = torch.zeros((512,))\n",
    "torch.save(visual_features, \"adaptations/data/zeroed-visual-features/clip_features.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dl4nlp_assignment_1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "05753275db4c417a6068616f81db8df1fa4ccdafd9acd0a9b6ad9f4706cb5748"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
