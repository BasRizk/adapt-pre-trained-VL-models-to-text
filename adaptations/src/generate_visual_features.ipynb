{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process visual features to get constant visual features for adaptations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Make sure that you are standing in the root folder of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/basem/projects/CSCI 566/Project/adapt-pre-trained-VL-models-to-text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/basem/projects/CSCI 566/Project/adapt-pre-trained-VL-models-to-text'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../../\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs all computations necessary to generate the constant visual features used for the following adaptations:\n",
    "* `avg-visual-features`\n",
    "* `zero-image-visual-features`\n",
    "* `zeroed-visual-features`\n",
    "\n",
    "LXMERT and VisualBERT use the same types of visual features, while CLIP-BERT doesn't. Thus, we generate two separate visual feature versions per adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from adaptations.src.utils import load_obj_tsv\n",
    "import torch\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From average over training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT\n",
    "\n",
    "First, you need to download the image datasets. Do this via the following commands:\n",
    "\n",
    "**MS COCO**\n",
    "\n",
    "Train\n",
    "```bash\n",
    "wget https://nlp.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/train2014_obj36.zip -P adaptations/data/lxmert/mscoco_imgfeat\n",
    "unzip adaptations/data/lxmert/mscoco_imgfeat/train2014_obj36.zip -d adaptations/data/lxmert/mscoco_imgfeat && rm adaptations/data/lxmert/mscoco_imgfeat/train2014_obj36.zip\n",
    "```\n",
    "* 17 GB zipped\n",
    "* 31 GB unzipped and downloaded\n",
    "\n",
    "Validation\n",
    "```bash\n",
    "wget https://nlp.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/val2014_obj36.zip -P adaptations/data/lxmert/mscoco_imgfeat\n",
    "unzip adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.zip -d adaptations/data/lxmert/mscoco_imgfeat && rm adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.zip\n",
    "```\n",
    "* 8.1 GB zipped\n",
    "* 15 GB unzipped and downloaded\n",
    "\n",
    "**Visual Genome**\n",
    "\n",
    "```bash\n",
    "wget https://nlp.cs.unc.edu/data/lxmert_data/vg_gqa_imgfeat/vg_gqa_obj36.zip -P adaptations/data/lxmert/vg_gqa_imgfeat\n",
    "unzip adaptations/data/lxmert/vg_gqa_imgfeat/vg_gqa_obj36.zip -d adaptations/data/lxmert/vg_gqa_imgfeat && rm adaptations/data/lxmert/vg_gqa_imgfeat/vg_gqa_obj36.zip\n",
    "```\n",
    "\n",
    "* 30 GB zipped\n",
    "* 55 GB unzipped and downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import base64\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "FIELDNAMES = [\"img_id\", \"img_h\", \"img_w\", \"objects_id\", \"objects_conf\",\n",
    "              \"attrs_id\", \"attrs_conf\", \"num_boxes\", \"boxes\", \"features\"]\n",
    "\n",
    "def get_tsv_data_item(item):\n",
    "    for key in ['img_h', 'img_w', 'num_boxes']:\n",
    "        item[key] = int(item[key])\n",
    "\n",
    "    boxes = item['num_boxes']\n",
    "    decode_config = [\n",
    "        ('objects_id', (boxes, ), np.int64),\n",
    "        ('objects_conf', (boxes, ), np.float32),\n",
    "        ('attrs_id', (boxes, ), np.int64),\n",
    "        ('attrs_conf', (boxes, ), np.float32),\n",
    "        ('boxes', (boxes, 4), np.float32),\n",
    "        ('features', (boxes, -1), np.float32),\n",
    "    ]\n",
    "    for key, shape, dtype in decode_config:\n",
    "        item[key] = np.frombuffer(base64.b64decode(item[key]), dtype=dtype)\n",
    "        item[key] = item[key].reshape(shape)\n",
    "        item[key].setflags(write=False)\n",
    "        \n",
    "    return item\n",
    "\n",
    "# LXMERT expects normalized boxes (copied from airsplay/lxmert)\n",
    "def get_normalized_boxes(item):\n",
    "    # Normalize the boxes (to 0 ~ 1)\n",
    "    img_h, img_w = item['img_h'], item['img_w']\n",
    "    boxes = item[\"boxes\"].copy()\n",
    "    boxes[:, (0, 2)] /= img_w\n",
    "    boxes[:, (1, 3)] /= img_h\n",
    "    np.testing.assert_array_less(boxes, 1+1e-5)\n",
    "    np.testing.assert_array_less(-boxes, 0+1e-5)\n",
    "    \n",
    "    return boxes\n",
    "    \n",
    "\n",
    "def get_avg_visual_properties_from_files(fnames, features_shape=(36, 2048), pos_shape=(36, 4)):\n",
    "    feature_vector = np.zeros(features_shape)\n",
    "    pos_vector = np.zeros(pos_shape)\n",
    "    num_iters = 0\n",
    "    for fname in fnames:\n",
    "        start_time = time.time()\n",
    "        print(\"Start to load Faster-RCNN detected objects from %s\" % fname)\n",
    "        with open(fname) as f:\n",
    "            reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "            for i, item in enumerate(tqdm(reader)):\n",
    "                item = get_tsv_data_item(item)\n",
    "                feature_vector += item[\"features\"]\n",
    "                pos_vector += get_normalized_boxes(item)\n",
    "                num_iters += 1\n",
    "                \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Loaded file %s in %d seconds.\" % (fname, elapsed_time))\n",
    "    return feature_vector/num_iters, pos_vector/num_iters\n",
    "\n",
    "def get_avg_visual_properties_across_detections_from_files(fnames, features_shape=(1, 2048), pos_shape=(1, 4)):\n",
    "    feature_vector = np.zeros(features_shape)\n",
    "    pos_vector = np.zeros(pos_shape)\n",
    "    num_iters = 0\n",
    "    for fname in fnames:\n",
    "        start_time = time.time()\n",
    "        print(\"Start to load Faster-RCNN detected objects from %s\" % fname)\n",
    "        with open(fname) as f:\n",
    "            reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "            for i, item in enumerate(tqdm(reader)):\n",
    "                item = get_tsv_data_item(item)\n",
    "                feature_vector += np.sum(item[\"features\"], axis=0)\n",
    "                pos_vector += np.sum(get_normalized_boxes(item), axis=0)\n",
    "                num_iters += item[\"features\"].shape[0]\n",
    "                \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Loaded file %s in %d seconds.\" % (fname, elapsed_time))\n",
    "    return feature_vector/num_iters, pos_vector/num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_VAL_FEATURES_PATH = \"adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.tsv\"\n",
    "COCO_TRAIN_FEATURES_PATH = \"adaptations/data/lxmert/mscoco_imgfeat/train2014_obj36.tsv\"\n",
    "VG_FEATURES_PATH = \"adaptations/data/lxmert/vg_gqa_imgfeat/vg_gqa_obj36.tsv\"\n",
    "\n",
    "data_files = [COCO_VAL_FEATURES_PATH, COCO_TRAIN_FEATURES_PATH, VG_FEATURES_PATH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per detection (one different vector for each detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load Faster-RCNN detected objects from adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.tsv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m avg_feature_vector, avg_pos_vector \u001b[39m=\u001b[39m get_avg_visual_properties_from_files(data_files, features_shape\u001b[39m=\u001b[39;49m(\u001b[39m36\u001b[39;49m, \u001b[39m2048\u001b[39;49m), pos_shape\u001b[39m=\u001b[39;49m(\u001b[39m36\u001b[39;49m, \u001b[39m4\u001b[39;49m))\n\u001b[1;32m      3\u001b[0m avg_feature_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(avg_feature_vector)\n\u001b[1;32m      4\u001b[0m torch\u001b[39m.\u001b[39msave(avg_feature_tensor, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39madaptations/data/avg-visual-features\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrcnn_features_per_detection.pt\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m, in \u001b[0;36mget_avg_visual_properties_from_files\u001b[0;34m(fnames, features_shape, pos_shape)\u001b[0m\n\u001b[1;32m     51\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart to load Faster-RCNN detected objects from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m fname)\n\u001b[0;32m---> 53\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(fname) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     54\u001b[0m     reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictReader(f, FIELDNAMES, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[39mfor\u001b[39;00m i, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(reader)):\n",
      "File \u001b[0;32m~/miniconda3/envs/vl/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'adaptations/data/lxmert/mscoco_imgfeat/val2014_obj36.tsv'"
     ]
    }
   ],
   "source": [
    "avg_feature_vector, avg_pos_vector = get_avg_visual_properties_from_files(data_files, features_shape=(36, 2048), pos_shape=(36, 4))\n",
    "\n",
    "avg_feature_tensor = torch.Tensor(avg_feature_vector)\n",
    "torch.save(avg_feature_tensor, os.path.join(\"adaptations/data/avg-visual-features\", \"frcnn_features_per_detection.pt\"))\n",
    "\n",
    "avg_pos_tensor = torch.Tensor(avg_pos_vector)\n",
    "torch.save(avg_pos_tensor, os.path.join(\"adaptations/data/avg-visual-features\", \"frcnn_boxes_per_detection.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'models/data/clip-bert/clip_features.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m buffer \u001b[39m=\u001b[39m h5py\u001b[39m.\u001b[39;49mFile(\u001b[39m\"\u001b[39;49m\u001b[39mmodels/data/clip-bert/clip_features.hdf5\u001b[39;49m\u001b[39m\"\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m image_features \u001b[39m=\u001b[39m buffer[\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/vl/lib/python3.8/site-packages/h5py/_hl/files.py:424\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mwith\u001b[39;00m phil:\n\u001b[1;32m    423\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m--> 424\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size,\n\u001b[1;32m    425\u001b[0m                    fapl, fcpl\u001b[39m=\u001b[39;49mmake_fcpl(track_order\u001b[39m=\u001b[39;49mtrack_order, fs_strategy\u001b[39m=\u001b[39;49mfs_strategy,\n\u001b[1;32m    426\u001b[0m                    fs_persist\u001b[39m=\u001b[39;49mfs_persist, fs_threshold\u001b[39m=\u001b[39;49mfs_threshold),\n\u001b[1;32m    427\u001b[0m                    swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[1;32m    429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m~/miniconda3/envs/vl/lib/python3.8/site-packages/h5py/_hl/files.py:190\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[1;32m    189\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 190\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[1;32m    191\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    192\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:96\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'models/data/clip-bert/clip_features.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "buffer = h5py.File(\"models/data/clip-bert/clip_features.hdf5\", mode=\"r\")\n",
    "image_features = buffer[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_vector = torch.zeros((512,))\n",
    "for feats in image_features:\n",
    "    avg_feature_vector += feats\n",
    "avg_feature_vector = avg_feature_vector/len(image_features)\n",
    "torch.save(avg_feature_vector, \"adaptations/data/avg-visual-features/clip_features.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For black (zeroed) image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT and VisualBERT (Faster-R CNN)\n",
    "\n",
    "The `output.csv` file is generated from a Faster-R CNN using the code in `adaptations/data/zero-image-visual-features`.\n",
    "\n",
    "ID translations can be found at \n",
    "* https://github.com/peteanderson80/bottom-up-attention/blob/master/data/genome/1600-400-20/objects_vocab.txt.\n",
    "* https://github.com/peteanderson80/bottom-up-attention/blob/master/data/genome/1600-400-20/attributes_vocab.txt\n",
    "\n",
    "**objects_id**\n",
    "\n",
    "72: \"sky\"\n",
    "\n",
    "956: \"background\"\n",
    "\n",
    "**attrs_id**\n",
    "\n",
    "11: \"black\"\n",
    "\n",
    "163: \"dark\"\n",
    "\n",
    "The Faster-R CNN results seemingly agree with results by Iki et al. for their black image (https://github.com/Alab-NII/eval_vl_glue/blob/main/demo/extractor_demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"adaptations/data/zero-image-visual-features/output.csv\") as f:\n",
    "    reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "    for i, item in enumerate(tqdm(reader)):\n",
    "        assert i < 1\n",
    "        item = get_tsv_data_item(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item[\"features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.Tensor(item[\"features\"]), \"adaptations/data/zero-image-visual-features/frcnn_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_boxes = get_normalized_boxes(item)\n",
    "torch.save(torch.Tensor(normalized_boxes), \"adaptations/data/zero-image-visual-features/frcnn_boxes.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor\n",
    "from PIL import Image\n",
    "\n",
    "from models.src.clip_bert.precompute_clip_visual_features import PatchedCLIPFeatureExtractor, VisualOnlyCLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor.feature_extractor = PatchedCLIPFeatureExtractor(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    pil_image = Image.open(\"adaptations/data/zero-image-visual-features/filled_with_0.png\")\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "    example = processor(images=pil_image, return_tensors=\"pt\")\n",
    "    example[\"pixel_values\"] = example[\"pixel_values\"]\n",
    "\n",
    "    model = VisualOnlyCLIPModel().to(device)\n",
    "    clip_features = model(**example.to(device))\n",
    "    clip_features = clip_features.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clip_features, \"adaptations/data/zero-image-visual-features/clip_features.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero vector as visual features filler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT and VisualBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_features = torch.zeros((36, 2048))\n",
    "visual_boxes = torch.zeros((36, 4))\n",
    "\n",
    "torch.save(visual_features, \"adaptations/data/zeroed-visual-features/frcnn_features.pt\")\n",
    "torch.save(visual_boxes, \"adaptations/data/zeroed-visual-features/frcnn_boxes.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_features = torch.zeros((512,))\n",
    "torch.save(visual_features, \"adaptations/data/zeroed-visual-features/clip_features.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dl4nlp_assignment_1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "05753275db4c417a6068616f81db8df1fa4ccdafd9acd0a9b6ad9f4706cb5748"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
