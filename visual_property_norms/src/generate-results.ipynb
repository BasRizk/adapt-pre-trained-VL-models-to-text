{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Visual Property Norms results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to investigate the effect of different adaptations to a text-only input for pre-trained vision-and-language (VL) models.\n",
    "\n",
    "This notebook is used for generating adaptation results on Visual Property Norms (VPN) for the BERT-base, LXMERT, VisualBERT and CLIP-BERT models. Visual Property Norms is a text-only tasks that more or less evaluates visual commonsense knowledge. LXMERT, VisualBERT and CLIP-BERT are VL models.\n",
    "\n",
    "The notebook is segmented into 8 main parts in which we evaluate different models and adaptations of these on VPN:\n",
    "1. All models as they are, with no or small adaptations to text-only input, queried without visual features\n",
    "2. BERT-base trained on Wikipedia or LXMERT text training data with no adaptation (used as a baseline)\n",
    "3. All VL models adapted to text-only input through finetuning on Wikipedia or LXMERT text training data, queried without visual features\n",
    "4. All VL models queried with constant average visual features from their training sets\n",
    "5. All VL models queried with constant visual features from a black image\n",
    "6. All VL models queried with constant visual features that are zeroes\n",
    "7. All VL models queried with constant visual features that have been finetuned to either Wikipedia or LXMERT text train data\n",
    "8. CLIP-BERT queried with visual features generated by CLIP based on the textual query (\"imagined visual features\")\n",
    "\n",
    "For each model and adaptation we save the results on VPN in a file.\n",
    "\n",
    "Lastly, all of the model and adaptation results are aggregated to one dataframe and saved to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup packages and save file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertConfig, CLIPModel, CLIPProcessor, VisualBertForPreTraining, VisualBertConfig, LxmertForPreTraining, LxmertConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from models.src.clip_bert.modeling_bert import BertImageForMaskedLM\n",
    "from models.src.lxmert.alterations import LxmertLanguageOnlyXLayer\n",
    "\n",
    "# import support functions\n",
    "from visual_property_norms.src.utils import get_model_results, get_model_preds_for_questions, get_clip_bert_model, get_clip_bert_preds_for_questions, get_lxmert_preds_for_questions, get_visualbert_preds_for_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FOLDER = \"visual_property_norms/data/results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluate all models as they are (more or less)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure with MAP. Report results per 1) pf split and 2) feature starter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"default\"\n",
    "model_name = \"BERT-base\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"default\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "model.eval()\n",
    "\n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"default\"\n",
    "model_name = \"VisualBERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"default\"\n",
    "model_name = \"LXMERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "prev_encoder = copy.deepcopy(model.lxmert.encoder)\n",
    "model.lxmert.encoder.x_layers = torch.nn.ModuleList([LxmertLanguageOnlyXLayer(model.lxmert.encoder.config) for _ in range(model.lxmert.encoder.config.x_layers)])\n",
    "model.lxmert.encoder.load_state_dict(prev_encoder.state_dict())\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT-base trained on Wikipedia or LXMERT text training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate BERT trained on LXMERT data from pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"trained-LXMERT\"\n",
    "model_name = \"BERT-base\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-lxmert-trained/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "   \n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions, batch_size=256)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate BERT trained on LXMERT data from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"trained-LXMERT-scratch\"\n",
    "model_name = \"BERT-base\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-lxmert-trained-scratch/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "   \n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-base trained on Wikipedia (same size as LXMERT data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"trained-Wikipedia\"\n",
    "model_name = \"BERT-base\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertImageForMaskedLM(config)\n",
    "model.load_state_dict(torch.load(\"models/data/model-weights/bert-wikipedia-trained/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. All VL models finetuned on Wikipedia or LXMERT text training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT on unimodal Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"no-visual-features-finetuned-Wikipedia\"\n",
    "model_name = \"LXMERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "config = LxmertConfig.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "model = LxmertForPreTraining(config)\n",
    "model.lxmert.encoder.x_layers = torch.nn.ModuleList([LxmertLanguageOnlyXLayer(model.lxmert.encoder.config) for _ in range(model.lxmert.encoder.config.x_layers)])\n",
    "model.load_state_dict(torch.load(\"adaptations/data/runs/finetune/lxmert-wikipedia/best_global_step1200/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions, batch_size=256)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualBERT on unimodal Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"no-visual-features-finetuned-Wikipedia\"\n",
    "model_name = \"VisualBERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "model = VisualBertForPreTraining(config)\n",
    "model.load_state_dict(torch.load(\"adaptations/data/runs/finetune/visualbert-wikipedia/best_global_step440/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions, batch_size=256)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT on unimodal Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"no-visual-features-finetuned-Wikipedia\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"adaptations/data/runs/finetune/clip-bert-wikipedia/best_global_step120/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "model.eval()\n",
    " \n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True, batch_size=256)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT on unimodal LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"no-visual-features-finetuned-LXMERT\"\n",
    "model_name = \"LXMERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "config = LxmertConfig.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "model = LxmertForPreTraining(config)\n",
    "model.lxmert.encoder.x_layers = torch.nn.ModuleList([LxmertLanguageOnlyXLayer(model.lxmert.encoder.config) for _ in range(model.lxmert.encoder.config.x_layers)])\n",
    "model.load_state_dict(torch.load(\"adaptations/data/runs/finetune/lxmert-lxmert/best_global_step400/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualBERT on unimodal LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"no-visual-features-finetuned-LXMERT\"\n",
    "model_name = \"VisualBERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "model = VisualBertForPreTraining(config)\n",
    "model.load_state_dict(torch.load(\"adaptations/data/runs/finetune/visualbert-lxmert/best_global_step600/mp_rank_00_model_states.pt\", map_location=\"cpu\")[\"module\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "get_preds = lambda questions: get_model_preds_for_questions(model, tokenizer, questions, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT on unimodal LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"no-visual-features-finetuned-LXMERT\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"adaptations/data/runs/finetune/clip-bert-lxmert/best_global_step320/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "model.eval()\n",
    " \n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use average visual features filler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"avg-visual-features\"\n",
    "model_name = \"LXMERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# features have dimension (36, 2048)\n",
    "visual_features = torch.load(\"adaptations/data/avg-visual-features/frcnn_features_per_detection.pt\")\n",
    "visual_boxes = torch.load(\"adaptations/data/avg-visual-features/frcnn_boxes_per_detection.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions, batch_size=128, visual_features=visual_features, visual_boxes=visual_boxes)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that VisualBERT image data from VisualBERT repo is not compatible with VisualBERT on Huggingface. Visual embedding dim of data from VisualBERT repo is 1024 (150, 1024), while VisualBERT config above says 2048. Therefore, we use the LXMERT visual features instead, it should be from the same backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"avg-visual-features\"\n",
    "model_name = \"VisualBERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Using LXMERT features below\n",
    "visual_features = torch.load(\"adaptations/data/avg-visual-features/frcnn_features_per_detection.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_visualbert_preds_for_questions(model, tokenizer, questions, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"avg-visual-features\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "model.eval()\n",
    "\n",
    "# features have dimension (512,)\n",
    "visual_features = torch.load(\"adaptations/data/avg-visual-features/clip_features.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use feature vector from black image as visual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"zero-image-visual-features\"\n",
    "model_name = \"LXMERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "batch_size = 64\n",
    "\n",
    "visual_features = torch.load(\"adaptations/data/zero-image-visual-features/frcnn_features.pt\")\n",
    "visual_boxes = torch.load(\"adaptations/data/zero-image-visual-features/frcnn_boxes.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions, batch_size=batch_size, visual_features=visual_features, visual_boxes=visual_boxes)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"zero-image-visual-features\"\n",
    "model_name = \"VisualBERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# VisualBERT features have dimension (150, 1024)\n",
    "# Using LXMERT features below!!\n",
    "visual_features = torch.load(\"adaptations/data/zero-image-visual-features/frcnn_features.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_visualbert_preds_for_questions(model, tokenizer, questions, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"zero-image-visual-features\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "model.eval()\n",
    "\n",
    "# features have dimension (512,)\n",
    "visual_features = torch.load(\"adaptations/data/zero-image-visual-features/clip_features.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use zero vectors as visual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"zeroed-visual-features\"\n",
    "model_name = \"LXMERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "batch_size = 64\n",
    "visual_features = torch.zeros((36, 2048))\n",
    "visual_boxes = torch.zeros((36, 4))\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions, batch_size=batch_size, visual_features=visual_features, visual_boxes=visual_boxes)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"zeroed-visual-features\"\n",
    "model_name = \"VisualBERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# VisualBERT features have dimension (150, 1024)\n",
    "# Using LXMERT features below\n",
    "visual_features = torch.zeros((36, 2048))\n",
    "\n",
    "get_preds = lambda questions: get_visualbert_preds_for_questions(model, tokenizer, questions, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"zeroed-visual-features\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "model.eval()\n",
    "\n",
    "# features have dimension (512,)\n",
    "visual_features = torch.zeros((512,))\n",
    "\n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use fine-tuned visual feature fillers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT - tuned on LXMERT-finetune (9,500 train samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"finetuned-LXMERT-visual-features\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "model.eval()\n",
    "\n",
    "# features have dimension (512,)\n",
    "visual_features = torch.load(\"adaptations/data/runs/finetune-visual-features/clip-bert-lxmert/best_global_step_features4520.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT - tuned on Wikipedia-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"finetuned-Wikipedia-visual-features\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=True)\n",
    "model.eval()\n",
    "\n",
    "# features have dimension (512,)\n",
    "visual_features = torch.load(\"adaptations/data/runs/finetune-visual-features/clip-bert-wikipedia/best_global_step_features75600.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=True, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT - tuned on LXMERT-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"finetuned-LXMERT-visual-features\"\n",
    "model_name = \"LXMERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "batch_size = 64\n",
    "\n",
    "visual_features = torch.load(\"adaptations/data/runs/finetune-visual-features/lxmert-lxmert/best_global_step_features70080.pt\")\n",
    "visual_boxes = torch.load(\"adaptations/data/runs/finetune-visual-features/lxmert-lxmert/best_global_step_boxes70080.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions, batch_size=batch_size, visual_features=visual_features, visual_boxes=visual_boxes)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT - tuned on Wikipedia-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"finetuned-Wikipedia-visual-features\"\n",
    "model_name = \"LXMERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "batch_size = 64\n",
    "\n",
    "visual_features = torch.load(\"adaptations/data/runs/finetune-visual-features/lxmert-wikipedia/best_global_step_features72320.pt\")\n",
    "visual_boxes = torch.load(\"adaptations/data/runs/finetune-visual-features/lxmert-wikipedia/best_global_step_boxes72320.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_lxmert_preds_for_questions(model, tokenizer, questions, batch_size=batch_size, visual_features=visual_features, visual_boxes=visual_boxes)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualBERT - tuned on LXMERT-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"finetuned-LXMERT-visual-features\"\n",
    "model_name = \"VisualBERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# VisualBERT features have dimension (150, 1024)\n",
    "# Using LXMERT shaped features below\n",
    "visual_features = torch.load(\"adaptations/data/runs/finetune-visual-features/visualbert-lxmert/best_global_step_features5920.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_visualbert_preds_for_questions(model, tokenizer, questions, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualBERT - tuned on Wikipedia-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"finetuned-Wikipedia-visual-features\"\n",
    "model_name = \"VisualBERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# VisualBERT features have dimension (150, 1024)\n",
    "# Using LXMERT shaped features below!!\n",
    "visual_features = torch.load(\"adaptations/data/runs/finetune-visual-features/visualbert-wikipedia/best_global_step_features4400.pt\")\n",
    "\n",
    "get_preds = lambda questions: get_visualbert_preds_for_questions(model, tokenizer, questions, batch_size=128, visual_features=visual_features)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Imagined visual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation = \"imagined-visual-features\"\n",
    "model_name = \"CLIP-BERT\"\n",
    "save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_path = \"models/data/model-weights/clip-bert/mp_rank_00_model_states.pt\"\n",
    "\n",
    "model, clip_model, clip_processor = get_clip_bert_model(model_path, no_visual_prediction=False)\n",
    "model.eval()\n",
    " \n",
    "get_preds = lambda questions: get_clip_bert_preds_for_questions(model, clip_model, clip_processor, questions, tokenizer, no_visual_prediction=False, batch_size=128)\n",
    "\n",
    "results = get_model_results(get_preds, tokenizer)\n",
    "results.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"BERT-base\",\n",
    "               \"CLIP-BERT\",\n",
    "               \"LXMERT\",\n",
    "               \"VisualBERT\"\n",
    "              ]\n",
    "adaptations = [\"default\",\n",
    "               \"trained-LXMERT\",\n",
    "               \"trained-LXMERT-scratch\",\n",
    "               \"trained-Wikipedia\",\n",
    "               \"no-visual-features-finetuned-LXMERT\",\n",
    "               \"no-visual-features-finetuned-Wikipedia\",\n",
    "               \"avg-visual-features\",\n",
    "               \"zero-image-visual-features\",\n",
    "               \"zeroed-visual-features\",\n",
    "               \"finetuned-LXMERT-visual-features\",\n",
    "               \"finetuned-Wikipedia-visual-features\",\n",
    "               \"imagined-visual-features\"\n",
    "              ]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for model_name in model_names:\n",
    "    for adaptation in adaptations:\n",
    "        save_filename = os.path.join(SAVE_FOLDER, get_save_filename(model_name, adaptation))\n",
    "        try:\n",
    "            tmp = pd.read_csv(save_filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't read results from model {model_name} with adaptation {adaptation}\")\n",
    "            print(\"This is expected if the model was never evaluated with this adaptation.\")\n",
    "            print(e)\n",
    "        else:\n",
    "            tmp[\"model\"] = model_name\n",
    "            tmp[\"adaptation\"] = adaptation\n",
    "            results = results.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(SAVE_FOLDER, \"results.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dl4nlp_assignment_1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "05753275db4c417a6068616f81db8df1fa4ccdafd9acd0a9b6ad9f4706cb5748"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
